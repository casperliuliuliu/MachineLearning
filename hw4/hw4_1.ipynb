{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/Users/liushiwen/Desktop/大四上/機器學習導論/MachineLearning/hw4/hw4_1.ipynb 儲存格 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m a1 \u001b[39m=\u001b[39m \u001b[39m3.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m a0 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m5.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m min_x \u001b[39m=\u001b[39m find_min(a4, a3, a2, a1, a0)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMinimum x:\u001b[39m\u001b[39m\"\u001b[39m, min_x)\n","\u001b[1;32m/Users/liushiwen/Desktop/大四上/機器學習導論/MachineLearning/hw4/hw4_1.ipynb 儲存格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):  \u001b[39m# 可以調整迭代次數\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# 清零梯度\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     equation\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# 計算梯度\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# 更新變數\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/liushiwen/Desktop/%E5%A4%A7%E5%9B%9B%E4%B8%8A/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%96/MachineLearning/hw4/hw4_1.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# 返回找到的最小值\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n","File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    206\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}],"source":["import torch\n","from torch.autograd import Variable\n","\n","def find_min(a4, a3, a2, a1, a0):\n","    # 定義變數 x，它是我們想要找到的最小值的位置\n","    x = Variable(torch.tensor(0.0), requires_grad=True)\n","\n","    # 定義方程式\n","    equation = a4 * x**4 + a3 * x**3 + a2 * x**2 + a1 * x + a0\n","\n","    # 使用 PyTorch 的優化器（這裡使用 SGD）來最小化方程式\n","    optimizer = torch.optim.SGD([x], lr=0.01)\n","\n","    # 訓練模型，即最小化方程式的值\n","    for _ in range(1000):  # 可以調整迭代次數\n","        optimizer.zero_grad()  # 清零梯度\n","        # equation.backward()  # 計算梯度\n","        optimizer.step()  # 更新變數\n","\n","    # 返回找到的最小值\n","    return x.item()\n","\n","# Example usage:\n","a4 = 1.0  # 請確保 a4 > 0\n","a3 = -2.0\n","a2 = 1.0\n","a1 = 3.0\n","a0 = -5.0\n","\n","min_x = find_min(a4, a3, a2, a1, a0)\n","print(\"Minimum x:\", min_x)\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
